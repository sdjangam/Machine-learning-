{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://hub.packtpub.com/4-ways-implement-feature-selection-python-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)\n",
    "\n",
    "Statistical tests can be used to select those features that have the strongest relationships with the output variable.\n",
    "\n",
    "The scikit-learn library provides the SelectKBest class, which can be used with a suite of different statistical tests to select a specific number of features.\n",
    "\n",
    "The following example uses the chi squared (chi^2) statistical test for non-negative features to select four of the best features from the Pima Indians onset of diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Import chi2 for performing chi square test\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "import pandas\n",
    "\n",
    "#URL for loading the dataset\n",
    "url =\"pima-indians-diabetes.data.csv\"\n",
    "#Define the attribute names\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "#Create pandas data frame by loading the data from URL\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "#Create array from data values\n",
    "array = dataframe.values\n",
    "#Split the data into input and target\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  111.51969064  1411.88704064    17.60537322    53.10803984  2175.56527292\n",
      "   127.66934333     5.39268155   181.30368904]\n"
     ]
    }
   ],
   "source": [
    "#We will select the features using chi square -- selects top 4.\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "#Fit the function for ranking the features by score\n",
    "fit = test.fit(X, Y)\n",
    "#Summarize scores\n",
    "print(fit.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.00000000e+00   1.48000000e+02   7.20000000e+01   3.50000000e+01\n",
      "    0.00000000e+00   3.36000000e+01   6.27000000e-01   5.00000000e+01]\n",
      " [  1.00000000e+00   8.50000000e+01   6.60000000e+01   2.90000000e+01\n",
      "    0.00000000e+00   2.66000000e+01   3.51000000e-01   3.10000000e+01]\n",
      " [  8.00000000e+00   1.83000000e+02   6.40000000e+01   0.00000000e+00\n",
      "    0.00000000e+00   2.33000000e+01   6.72000000e-01   3.20000000e+01]\n",
      " [  1.00000000e+00   8.90000000e+01   6.60000000e+01   2.30000000e+01\n",
      "    9.40000000e+01   2.81000000e+01   1.67000000e-01   2.10000000e+01]\n",
      " [  0.00000000e+00   1.37000000e+02   4.00000000e+01   3.50000000e+01\n",
      "    1.68000000e+02   4.31000000e+01   2.28800000e+00   3.30000000e+01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(768, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 148.     0.    33.6   50. ]\n",
      " [  85.     0.    26.6   31. ]\n",
      " [ 183.     0.    23.3   32. ]\n",
      " [  89.    94.    28.1   21. ]\n",
      " [ 137.   168.    43.1   33. ]]\n"
     ]
    }
   ],
   "source": [
    "#Apply the transformation on to dataset\n",
    "print(X[0:5,:])\n",
    "X.shape\n",
    "features = fit.transform(X)\n",
    "features.shape\n",
    "#Summarize selected features (selects 4 best features)\n",
    "#You can see the scores for each attribute and the four attributes \n",
    "# chosen (those with the highest scores): plas, test, mass, and age.\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Recursive Feature Elimination -- wrapper\n",
    "\n",
    "RFE works by recursively removing attributes and building a model on attributes that remain. It uses model accuracy to identify which attributes (and combinations of attributes) contribute the most to predicting the target attribute.\n",
    "\n",
    "The following example uses RFE with the logistic regression algorithm to select the top three features. The choice of algorithm does not matter too much as long as it is skillful and consistent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "url =\"pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 4\n",
      "Selected Features: [ True  True False False False  True  True False]\n",
      "Feature Ranking: [1 1 2 4 5 1 1 3]\n"
     ]
    }
   ],
   "source": [
    "#Feature extraction\n",
    "model = LogisticRegression() \n",
    "rfe = RFE(model, 4)\n",
    "fit = rfe.fit(X, Y)\n",
    "#You can see that RFE chose the the top three features as preg, mass, and pedi.\n",
    "print(\"Num Features: %d\"% fit.n_features_) \n",
    "print(\"Selected Features: %s\"% fit.support_) \n",
    "print(\"Feature Ranking: %s\"% fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------\n",
    "Choosing important features (feature importance)\n",
    "--------------------------------------------------\n",
    "\n",
    "Feature importance is the technique used to select features using a trained supervised classifier. When we train a classifier such as a decision tree, we evaluate each attribute to create splits; we can use this measure as a feature selector. Let’s understand it in detail.\n",
    "\n",
    "Random forests are among the most popular machine learning methods thanks to their relatively good accuracy, robustness, and ease of use. They also provide two straightforward methods for feature selection—mean decrease impurity and mean decrease accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otto Train data\n",
    "\n",
    "You can download training dataset, train.csv.zip, from the https://www.kaggle.com/c/otto-group-product-classification-challenge/data and place the unzipped train.csv file in your working directory.\n",
    "\n",
    "This dataset describes 93 obfuscated details of more than 61,000 products grouped into 10 product categories (for example, fashion, electronics, and so on). Input attributes are the counts of different events of some kind.\n",
    "\n",
    "The goal is to make predictions for new products as an array of probabilities for each of the 10 categories, and models are evaluated using multiclass logarithmic loss (also called cross entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to create Train and Test set from the original dataset \n",
    "def getTrainTestData(dataset,split):\n",
    "    np.random.seed(0) \n",
    "    training = [] \n",
    "    testing = []\n",
    "    np.random.shuffle(dataset) \n",
    "    shape = np.shape(dataset)\n",
    "    trainlength = np.uint16(np.floor(split*shape[0]))\n",
    "    for i in range(trainlength): \n",
    "        training.append(dataset[i])\n",
    "    for i in range(trainlength,shape[0]): \n",
    "        testing.append(dataset[i])\n",
    "    training = np.array(training) \n",
    "    testing = np.array(testing)\n",
    "    return training,testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to evaluate model performance\n",
    "def getAccuracy(pre,ytest): \n",
    "    count = 0\n",
    "    for i in range(len(ytest)):\n",
    "        if ytest[i]==pre[i]: \n",
    "            count+=1\n",
    "    acc = float(count)/len(ytest)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset  (35000, 94)\n",
      "Size of Data set before feature selection: 26.32 MB\n"
     ]
    }
   ],
   "source": [
    "#Load dataset as pandas data frame\n",
    "data = read_csv('ottoTrain.csv')\n",
    "#Extract attribute names from the data frame\n",
    "feat = data.keys()\n",
    "feat_labels = feat.get_values()\n",
    "#Extract data values from the data frame\n",
    "dataset = data.values\n",
    "#Shuffle the dataset\n",
    "np.random.shuffle(dataset)\n",
    "#We will select 50000 instances to train the classifier\n",
    "inst = 50000\n",
    "#Extract 50000 instances from the dataset\n",
    "dataset = dataset[0:inst,:]\n",
    "#Create Training and Testing data for performance evaluation\n",
    "train,test = getTrainTestData(dataset, 0.7)\n",
    "#Split data into input and output variable with selected features\n",
    "Xtrain = train[:,0:94] \n",
    "ytrain = train[:,94] \n",
    "shape = np.shape(Xtrain)\n",
    "print(\"Shape of the dataset \",shape)\n",
    "#Print the size of Data in MBs\n",
    "print(\"Size of Data set before feature selection:\",(Xtrain.nbytes/1e6),\"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=30, max_features=7, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for building the Tree is: 14.783088\n",
      "Accuracy of model before feature selection is 98.82\n"
     ]
    }
   ],
   "source": [
    "#Lets select the test data for model evaluation purpose\n",
    "Xtest = test[:,0:94] \n",
    "ytest = test[:,94]\n",
    "#Create a random forest classifier with the following Parameters\n",
    "trees= 250\n",
    "max_feat= 7\n",
    "max_depth = 30\n",
    "min_sample = 2\n",
    "clf = RandomForestClassifier(n_estimators=trees, max_features=max_feat, max_depth=max_depth, \n",
    "min_samples_split= min_sample, random_state=0,n_jobs=-1)\n",
    "#Train the classifier and calculate the training time\n",
    "import time\n",
    "start = time.time() \n",
    "clf.fit(Xtrain, ytrain) \n",
    "end = time.time()\n",
    "#Lets Note down the model training time\n",
    "print(\"Execution time for building the Tree is: %f\"%(float(end)- float(start)))\n",
    "pre = clf.predict(Xtest)\n",
    "#Let's see how much time is required to train the model on the training dataset:\n",
    "#Evaluate the model performance for the test data\n",
    "acc = getAccuracy(pre, ytest)\n",
    "print(\"Accuracy of model before feature selection is\",(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('id', 0.33346650420175183)\n",
      "('feat_1', 0.0036186958628801214)\n",
      "('feat_2', 0.0037243050888530957)\n",
      "('feat_3', 0.011579217472062748)\n",
      "('feat_4', 0.010297382675187445)\n",
      "('feat_5', 0.0010359139416194116)\n",
      "('feat_6', 0.00038171336038056165)\n",
      "('feat_7', 0.0024867672489765021)\n",
      "('feat_8', 0.0096689721610546085)\n",
      "('feat_9', 0.007906150362995093)\n",
      "('feat_10', 0.0022342480802130366)\n",
      "('feat_11', 0.030321202266427427)\n",
      "('feat_12', 0.001120862950070666)\n",
      "('feat_13', 0.0039919844660730253)\n",
      "('feat_14', 0.019408706880663498)\n",
      "('feat_15', 0.015398634496632809)\n",
      "('feat_16', 0.0055203970543115446)\n",
      "('feat_17', 0.0071982339042675871)\n",
      "('feat_18', 0.0036309310056707512)\n",
      "('feat_19', 0.0038008858005607127)\n",
      "('feat_20', 0.0046001001637091758)\n",
      "('feat_21', 0.0012839572570891803)\n",
      "('feat_22', 0.003458048185607362)\n",
      "('feat_23', 0.0019414256864660538)\n",
      "('feat_24', 0.009502403878816023)\n",
      "('feat_25', 0.018382070498456828)\n",
      "('feat_26', 0.022011162365845233)\n",
      "('feat_27', 0.0082921478476573572)\n",
      "('feat_28', 0.0031557384078345616)\n",
      "('feat_29', 0.0024792257598606751)\n",
      "('feat_30', 0.0066476239193098453)\n",
      "('feat_31', 0.0012599923643107668)\n",
      "('feat_32', 0.008187326942297634)\n",
      "('feat_33', 0.0056088907066336752)\n",
      "('feat_34', 0.036628469546452588)\n",
      "('feat_35', 0.0063498841547460555)\n",
      "('feat_36', 0.013851450124186428)\n",
      "('feat_37', 0.0033450862421822323)\n",
      "('feat_38', 0.0048662486853085678)\n",
      "('feat_39', 0.0083582925240580589)\n",
      "('feat_40', 0.019564634122549943)\n",
      "('feat_41', 0.0037385207530162496)\n",
      "('feat_42', 0.016443622996113041)\n",
      "('feat_43', 0.0065833427677405368)\n",
      "('feat_44', 0.0029710175097328465)\n",
      "('feat_45', 0.0031412372400914258)\n",
      "('feat_46', 0.0074046769199337297)\n",
      "('feat_47', 0.005484716424559311)\n",
      "('feat_48', 0.0079174343111174817)\n",
      "('feat_49', 0.0024748089590100699)\n",
      "('feat_50', 0.0055491610952644157)\n",
      "('feat_51', 0.00077807101535367457)\n",
      "('feat_52', 0.0013770978966777152)\n",
      "('feat_53', 0.0062838201764186001)\n",
      "('feat_54', 0.0070409133660147821)\n",
      "('feat_55', 0.0027637347016962236)\n",
      "('feat_56', 0.003367563354613455)\n",
      "('feat_57', 0.010034132033680178)\n",
      "('feat_58', 0.0048366509075695348)\n",
      "('feat_59', 0.0065398342805150908)\n",
      "('feat_60', 0.029232707820944597)\n",
      "('feat_61', 0.0082276192783272868)\n",
      "('feat_62', 0.01265411944923474)\n",
      "('feat_63', 0.0013136663846220629)\n",
      "('feat_64', 0.0077248047713444025)\n",
      "('feat_65', 0.0016124315206767128)\n",
      "('feat_66', 0.004058900327604485)\n",
      "('feat_67', 0.014128729007342087)\n",
      "('feat_68', 0.0078441724228614507)\n",
      "('feat_69', 0.011113080319938719)\n",
      "('feat_70', 0.0048879115207359561)\n",
      "('feat_71', 0.0049887388367168056)\n",
      "('feat_72', 0.0076514066187613429)\n",
      "('feat_73', 0.0029274467052192005)\n",
      "('feat_74', 0.0023259396168049634)\n",
      "('feat_75', 0.011811733886280302)\n",
      "('feat_76', 0.0083503649921964126)\n",
      "('feat_77', 0.0016130280209724021)\n",
      "('feat_78', 0.0074770429328223529)\n",
      "('feat_79', 0.0038636267679849653)\n",
      "('feat_80', 0.0071340513739710639)\n",
      "('feat_81', 0.0012958121098888356)\n",
      "('feat_82', 0.0026819501244384801)\n",
      "('feat_83', 0.0055882130879205188)\n",
      "('feat_84', 0.00086668575950532491)\n",
      "('feat_85', 0.0052523192915130655)\n",
      "('feat_86', 0.013716186370718413)\n",
      "('feat_87', 0.0034892173392352226)\n",
      "('feat_88', 0.0083077594844808717)\n",
      "('feat_89', 0.0024315803794271941)\n",
      "('feat_90', 0.014139709447807007)\n",
      "('feat_91', 0.0040490533687412584)\n",
      "('feat_92', 0.0047350648419810263)\n",
      "('feat_93', 0.0012086804458313768)\n"
     ]
    }
   ],
   "source": [
    "#Once we have trained the model we will rank all the features \n",
    "for feature in zip(feat_labels, clf.feature_importances_):\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=30, max_features=7, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "        norm_order=1, prefit=False, threshold=0.01)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select features which have higher contribution in the final prediction\n",
    "sfm = SelectFromModel(clf, threshold=0.01) \n",
    "sfm.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Data set before feature selection:  5.6  MB\n",
      "Shape of the dataset  (35000, 20)\n"
     ]
    }
   ],
   "source": [
    "#Transform input dataset\n",
    "Xtrain_1 = sfm.transform(Xtrain) \n",
    "Xtest_1= sfm.transform(Xtest)\n",
    "#Let's see the size and shape of new dataset \n",
    "print(\"Size of Data set before feature selection: \",(Xtrain_1.nbytes/1e6),\" MB\")\n",
    "shape = np.shape(Xtrain_1)\n",
    "print(\"Shape of the dataset \",shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=30, max_features=7, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for building the Random Forest is:  6.987008094787598\n",
      "Accuracy after feature selection  99.97333333333333\n"
     ]
    }
   ],
   "source": [
    "#Model training time\n",
    "start = time.time() \n",
    "clf.fit(Xtrain_1, ytrain) \n",
    "end = time.time()\n",
    "print(\"Execution time for building the Random Forest is: \",(float(end)- float(start)))\n",
    "#Let's evaluate the model on test data\n",
    "pre = clf.predict(Xtest_1) \n",
    "count = 0\n",
    "acc2 = getAccuracy(pre, ytest)\n",
    "print(\"Accuracy after feature selection \",(100*acc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection using Variance Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#http://scikit-learn.org/stable/modules/feature_selection.html\n",
    "#Removing features with low variance\n",
    "#VarianceThreshold is a simple baseline approach to feature selection. \n",
    "#It removes all features whose variance doesn’t meet some threshold. \n",
    "#By default, it removes all zero-variance features, \n",
    "#i.e. features that have the same value in all samples.\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "#suppose that we have a dataset with boolean features, and we want to remove all features\n",
    "#that are either one or zero (on or off) in more than 80% of the samples.\n",
    "X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X)\n",
    "#VarianceThreshold has removed the first column, which has a probability p = 5/6 > .8 of containing a zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
